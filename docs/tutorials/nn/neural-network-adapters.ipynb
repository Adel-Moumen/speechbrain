{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82d21c0-4bdb-4caf-af73-30d0f43ecbab",
   "metadata": {},
   "source": [
    "# Neural Network Adapters for faster low-memory fine-tuning\n",
    "\n",
    "This tutorial covers the SpeechBrain implementation of adapters such as LoRA. This includes how to integrate either SpeechBrain implemented adapters, custom adapters, and adapters from libraries such as PEFT into a pre-trained model.\n",
    "\n",
    "## Prerequisite\n",
    "- [Speech Recognition From Scratch](https://speechbrain.readthedocs.io/en/latest/tutorials/tasks/speech-recognition-from-scratch.html)\n",
    "\n",
    "## Introduction and Background\n",
    "\n",
    "As pre-trained models become larger and more capable, there is growing interest in methods for adapting them for specific tasks in a memory-efficient way, within a reasonable time span. One such technique is freezing the original parameters and inserting a small number of additional parameters into the original model, which are called \"adapters.\" These adapters can often match the performance of full fine-tuning at a fraction of the parameter count, meaning faster and more memory-efficient fine-tuning [1]. One popular technique for doing this is known as Low-Rank Adaptation (LoRA) [2].\n",
    "\n",
    "On the software side, HuggingFace has produced a popular library for adapters called PEFT [3]. Our implementation includes some of the features of this library, as well as including the ability to integrate PEFT adapters into a SpeechBrain model. To explore this further, let's proceed with the installation of SpeechBrain.\n",
    "\n",
    "### Relevant bibliography\n",
    "1. N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, \"Parameter-efficient transfer learning for NLP.\" In *International Conference on Machine Learning*, 2019.\n",
    "2. E.J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \"LoRA: Low-rank adaptation of large language models.\" In *International Conference on Learning Representations*, 2021.\n",
    "3. S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul, and  B. Bossan, \"PEFT: State-of-the-art parameter-efficient fine-tuning methods.\" *GitHub Repository*, 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45871e4-e087-486b-af01-411603e43070",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!python -m pip install git+https://github.com/speechbrain/speechbrain.git@develop\n",
    "\n",
    "!git clone https://github.com/speechbrain/speechbrain.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca49a1d-5c1b-40db-b7e1-dcc8a85bcce3",
   "metadata": {},
   "source": [
    "## Simple Fine-tuning\n",
    "\n",
    "We'll first show how to use adapters on a template recipe, which includes everything necessary for full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dabda46-19ae-41b9-ab0f-6b9995b1e27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.seed:Setting seed to 2602\n",
      "WARNING:speechbrain.utils.train_logger:torchvision is not available - cannot save figures\n",
      "/home/competerscience/Documents/Repositories/speechbrain/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/CRDNN_BPE_960h_LM/2602\n",
      "mini_librispeech_prepare - Creating ../train.json, ../valid.json, and ../test.json\n",
      "mini_librispeech_prepare - Transcription files read!\n",
      "../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac\n",
      "True\n",
      "/home/competerscience/Documents/Repositories/speechbrain/docs/tutorials/nn/speechbrain/templates/speech_recognition/ASR\n",
      "speechbrain.core - Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/competerscience/Documents/Repositories/speechbrain/docs/tutorials/nn/speechbrain/templates/speech_recognition/ASR/train.py\", line 454, in <module>\n",
      "    sb.utils.distributed.run_on_main(\n",
      "  File \"/home/competerscience/Documents/Repositories/speechbrain/speechbrain/utils/distributed.py\", line 105, in run_on_main\n",
      "    main_process_only(func)(*args, **kwargs)\n",
      "  File \"/home/competerscience/Documents/Repositories/speechbrain/speechbrain/utils/distributed.py\", line 168, in main_proc_wrapped_func\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/competerscience/Documents/Repositories/speechbrain/docs/tutorials/nn/speechbrain/templates/speech_recognition/ASR/mini_librispeech_prepare.py\", line 81, in prepare_mini_librispeech\n",
      "    create_json(wav_list_train, trans_dict, save_json_train)\n",
      "  File \"/home/competerscience/Documents/Repositories/speechbrain/docs/tutorials/nn/speechbrain/templates/speech_recognition/ASR/mini_librispeech_prepare.py\", line 136, in create_json\n",
      "    signal = read_audio(wav_file)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/competerscience/Documents/Repositories/speechbrain/speechbrain/dataio/dataio.py\", line 278, in read_audio\n",
      "    audio, _ = torchaudio.load(waveforms_obj)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/competerscience/Documents/uvenv/lib/python3.12/site-packages/torchaudio/_backend/utils.py\", line 205, in load\n",
      "    return backend.load(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/competerscience/Documents/uvenv/lib/python3.12/site-packages/torchaudio/_backend/sox.py\", line 44, in load\n",
      "    ret = sox_ext.load_audio_file(uri, frame_offset, num_frames, normalize, channels_first, format)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/competerscience/Documents/uvenv/lib/python3.12/site-packages/torch/_ops.py\", line 1061, in __call__\n",
      "    return self_._op(*args, **(kwargs or {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Error loading audio file: failed to open file ../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac\n"
     ]
    }
   ],
   "source": [
    "#%cd speechbrain/templates/speech_recognition/ASR\n",
    "!python train.py train.yaml --number_of_epochs=1  --batch_size=2  --enable_add_reverb=False --enable_add_noise=False #To speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bce8cd4-587a-4297-bb51-3c652922c2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c059ad-8a5d-4de8-903c-2456289357d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/competerscience/Documents/Repositories/speechbrain/docs/tutorials/nn/speechbrain/templates/speech_recognition/ASR'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7ae5b6-6d6c-451a-8ce8-1ac5fe4c7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccce0843-9f9b-43f1-b569-9890ac106ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror:\u001b[0m unrecognized subcommand '\u001b[33minstall\u001b[0m'\n",
      "\n",
      "  \u001b[32mtip:\u001b[0m a similar subcommand exists: '\u001b[32muv pip install\u001b[0m'\n",
      "\n",
      "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1muv\u001b[0m [OPTIONS] <COMMAND>\n",
      "\n",
      "For more information, try '\u001b[1m--help\u001b[0m'.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error loading audio file: failed to open file ../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muv install ffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/uvenv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/uvenv/lib/python3.12/site-packages/torchaudio/_backend/sox.py:44\u001b[0m, in \u001b[0;36mSoXBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoX backend does not support loading from file-like objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use an alternative backend that does support loading from file-like objects, e.g. FFmpeg.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43msox_ext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_audio_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/uvenv/lib/python3.12/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading audio file: failed to open file ../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac"
     ]
    }
   ],
   "source": [
    "!uv install ffmpeg\n",
    "torchaudio.load(\"../data/LibriSpeech/train-clean-5/669/129074/669-129074-0015.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9de34-e7d2-4fdd-beb7-d7ab07551c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
