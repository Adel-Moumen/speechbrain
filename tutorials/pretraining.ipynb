{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1228U7DsXZ3A_B3tkZoQhixC0WP6O0DZH","timestamp":1615161894564}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"79ryKiGHinQ3"},"source":["[![GitHub Repo stars](https://img.shields.io/github/stars/speechbrain/speechbrain?style=social) *Please, help our community project. Star on GitHub!*](https://github.com/speechbrain/speechbrain)\n","\n","# Pretrained Models and Fine-Tuning with <img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"40\"/>\n","\n","Training DNN models is often very time-consuming and expensive.\n","For this reason, whenever it is possible, using off-the-shelf pretrained models can be convenient in various scenarios.\n","\n","In SpeechBrain we provide pre-trained models and we also encourage users to share their own using <img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/>[HuggingFace Hub](https://huggingface.co/models)<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/> as we strongly believe that sharing models can help research.\n","\n","You can browse our official pre-trained models [here](https://huggingface.co/speechbrain).\n","\n","If you have a pre-trained model and want to include it among the official ones, please consider opening a pull request on [GitHub](https://github.com/speechbrain/speechbrain/blob/develop/README.md) with all the details of your model!\n","\n","We provide a simple and straightforward way to download and instantiate a state-of-the-art pretrained-model and use it either for direct inference or for fine-tuning/knowledge distillation or whatever new fancy technique you can come up with!\n","\n","With this tutorial, you will learn how to:\n","\n","1. Use pretrained models to infer on your data.\n","2. Use pretrained models as a component of a new pipeline (e.g language models, finetuning, speaker embeddings extraction ...).\n","\n","## Prerequisites\n","- [SpeechBrain Introduction](https://colab.research.google.com/drive/12bg3aUdr9mTfOGqcB5pSMABoIKPgiwcM?usp=sharing)\n","- [YAML tutorial](https://colab.research.google.com/drive/1Pg9by4b6-8QD2iC0U7Ic3Vxq4GEwEdDz?usp=sharing)\n","- [Brain Class tutorial](https://colab.research.google.com/drive/1fdqTk4CTXNcrcSVFvaOKzRfLmj4fJfwa?usp=sharing)\n","- [DataIOBasics](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH)\n"]},{"cell_type":"markdown","metadata":{"id":"HND7bB_1S07R"},"source":["## Installing Dependencies"]},{"cell_type":"code","source":["%%capture\n","# Installing SpeechBrain via pip\n","BRANCH = 'develop'\n","!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH"],"metadata":{"id":"BAzQuVzl6ww1"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pC-98RDOqPq"},"source":["%%capture\n","import speechbrain\n","# here we download the material needed for this tutorial: images and an example based on mini-librispeech\n","!wget https://www.dropbox.com/s/b61lo6gkpuplanq/MiniLibriSpeechTutorial.tar.gz?dl=0\n","!tar -xvzf MiniLibriSpeechTutorial.tar.gz?dl=0\n","# downloading mini_librispeech dev data\n","!wget https://www.openslr.org/resources/31/dev-clean-2.tar.gz\n","!tar -xvzf dev-clean-2.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KCk7uODNgNB"},"source":["## Using PreTrained models to perform inference on your data\n","\n","In this section, we will provide examples on using pretrained models with various tasks including:\n","1. Automatic Speech Recognition.\n","2. Speaker Recognition, Verification and Diarization.\n","3. Source Separation\n","\n","**Many more can be found in our <img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/>[HuggingFace Hub](https://huggingface.co/models)<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/>!**"]},{"cell_type":"markdown","metadata":{"id":"m0xCb38O6kFM"},"source":["### Automatic Speech Recognition"]},{"cell_type":"markdown","metadata":{"id":"PaWLPvJ2Nrxm"},"source":["Suppose we want to try out a pretrained ASR model on your data.\n","\n","Maybe we want to see if our new speech enhancement algorithm is able to also improve Word Error Rate or maybe we just want to transcribe a lecture recording.\n","\n","After browsing models on <img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/>[HuggingFace](https://huggingface.co/models)<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/>, we choose the following ASR pipeline: [super cool and SOTA ASR pipeline](https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech), trained on LibriSpeech.\n","\n","This ASR pipeline is made of three components as detailed [here](https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech):\n","\n","\n","1.   a CRDNN-based seq2seq E2E ASR model trained following this [recipe](https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech/ASR/seq2seq).\n","2.   a RNN-based LM.\n","3.   a SentencePiece Tokenizer object needed transforms words into subword units.\n","\n","**Now, we can transcribe any audio file in solely 3 lines of code!**\n"]},{"cell_type":"code","metadata":{"id":"GIJgefQpNnVO"},"source":["from speechbrain.inference.ASR import EncoderDecoderASR\n","asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"./pretrained_ASR\")\n","asr_model.transcribe_file(\"./LibriSpeech/dev-clean-2/1272/135031/1272-135031-0003.flac\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuULuuACaPfl"},"source":["We can validate this result by looking at the oracle transcription for this utterance."]},{"cell_type":"code","metadata":{"id":"JQ4IIIS8Z_ld"},"source":["!head ./LibriSpeech/dev-clean-2/1272/135031/1272-135031.trans.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHtw-fAJaX0A"},"source":["See, easy!"]},{"cell_type":"markdown","metadata":{"id":"yWcM9NSNE_uX"},"source":["**NOTE**\n","\n","This syntax can be used also to load models which are in the local filesystem.\n","In fact, the model is not re-downloaded a second time if it is already found in the local `savedir`."]},{"cell_type":"code","metadata":{"id":"M8k-O63qE-j6"},"source":["from speechbrain.inference.ASR import EncoderDecoderASR\n","asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"./pretrained_ASR\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVy5l-QT6qT3"},"source":["### Speaker Verification, Recognition and Diarization"]},{"cell_type":"markdown","metadata":{"id":"KQS7Sv4rUAwh"},"source":["Now suppose we want to perform another task such as diarization and we need a speaker embedding extractor.\n","\n","Fortunately, we can use directly the pretrained ECAPA TDNN model available [here](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb). The model is trained with [Voxceleb 2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/) and we can use it to extract speaker embeddings for each utterance:\n"]},{"cell_type":"code","metadata":{"id":"sNO_28zyT5OS"},"source":["from speechbrain.inference.speaker import SpeakerRecognition\n","import torchaudio\n","verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"./pretrained_ecapa\")\n","signal, fs = torchaudio.load('./LibriSpeech/dev-clean-2/1272/135031/1272-135031-0003.flac')\n","embedding = verification.encode_batch(signal)\n","embedding.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7AKrsLECPE2"},"source":["We can visualize how this DNN is able to extract embeddings which are representative of the speaker identity with a **PCA**."]},{"cell_type":"markdown","metadata":{"id":"lnC6v3-9Cak8"},"source":["We select 20 random utterances from MiniLibriSpeech corpus:"]},{"cell_type":"code","metadata":{"id":"54P7K8bOljwB"},"source":["import glob\n","import numpy as np\n","utterances = glob.glob(\"./LibriSpeech/dev-clean-2/**/*.flac\", recursive=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLA8puMW_xOF"},"source":["np.random.shuffle(utterances)\n","utterances = utterances[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUsg7YgZCgVs"},"source":["We extract embeddings on the whole utterances and put corresponding oracle speaker IDs on a list."]},{"cell_type":"code","metadata":{"id":"WAK74neqmryW"},"source":["from pathlib import Path\n","\n","embeddings = []\n","labels = []\n","for u in utterances:\n","    tmp, fs = torchaudio.load(u)\n","    e = verification.encode_batch(tmp)\n","    embeddings.append(e[0, 0].numpy())\n","    spk_label = Path(u).parent.parent.stem\n","    labels.append(spk_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ozhSGKS_CuRF"},"source":["We can use sklearn PCA for visualization."]},{"cell_type":"code","metadata":{"id":"bfssZX_k_1VW"},"source":["from sklearn.decomposition import PCA\n","\n","embeddings = np.array(embeddings)\n","\n","pca = PCA(n_components=2)\n","principalComponents = pca.fit_transform(embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grjNqihhCykb"},"source":["Finally we can plot the results. It can be seen that some speakers are clustered well even when visualizing only two components with PCA:"]},{"cell_type":"code","metadata":{"id":"j1U0N9b2ABG2"},"source":["import matplotlib.pyplot as plt\n","fig, ax = plt.subplots()\n","ax.scatter(principalComponents[:, 0], principalComponents[:, 1])\n","\n","for i, spkid in enumerate(labels):\n","    ax.annotate(spkid, (principalComponents[i, 0], principalComponents[i, 1]))\n","plt.xlabel(\"Principal Component 1\")\n","plt.ylabel(\"Principal Component 2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rDfkPAUA_JUk"},"source":["Given the embeddings computed with the ECAPA-TDNN model, we can perform speaker verification in this way:"]},{"cell_type":"code","metadata":{"id":"KuWnaG-j_Tr3"},"source":["# Different files from the same speaker\n","file1 = './LibriSpeech/dev-clean-2/1272/135031/1272-135031-0000.flac' # Same speaker\n","file2 = './LibriSpeech/dev-clean-2/1272/141231/1272-141231-0004.flac' # Same speaker\n","file3 = './LibriSpeech/dev-clean-2/1462/170142/1462-170142-0000.flac'  # Different speaker\n","\n","# Test with 2 files from the same speaker\n","score, prediction = verification.verify_files(file1, file2)\n","print(score, prediction)\n","\n","# Test with 2 files from  different speakers\n","score, prediction = verification.verify_files(file1, file3)\n","print(score, prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSax2FUVBiRu"},"source":["LibriSpeech is a very easy task for speaker verification. However, the ECAPA model works very well on other types of data. With voxceleb, we achieved an Equal Error Rate of 0.69%. Feel free to record yourself  (with a sampling rate of 16 kHz) and play with that!"]},{"cell_type":"markdown","metadata":{"id":"2AV_AknNQebh"},"source":["### Source Separation"]},{"cell_type":"markdown","metadata":{"id":"OFiEZitTaxcx"},"source":["What about Source Separation ?\n","A pretrained SepFormer model is available [here]:(https://huggingface.co/speechbrain/sepformer-wsj02mix). It can be used right out off the box to perform separation on clean speech mixtures.\n"]},{"cell_type":"markdown","metadata":{"id":"08KQG5rVQkaA"},"source":["We create an artificial mixture here by mixing together two utterances from MiniLibriSpeech."]},{"cell_type":"code","metadata":{"id":"SyEQ3QmmQR3H"},"source":["import torchaudio\n","s1, fs = torchaudio.load('./LibriSpeech/dev-clean-2/1272/135031/1272-135031-0003.flac')\n","s2, fs = torchaudio.load('./LibriSpeech/dev-clean-2/1462/170142/1462-170142-0001.flac')\n","\n","# we resample because we will use a model trained on 8KHz data.\n","resampler = torchaudio.transforms.Resample(fs, 8000)\n","s1 = resampler(s1)\n","s2 = resampler(s2)\n","fs= 8000\n","\n","min_len = min(s1.shape[-1], s2.shape[-1])\n","s1 = s1[:, :min_len]\n","s2 = s2[:, :min_len]\n","mix = s1 + s2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jBzCNMtRbdJ"},"source":["import IPython.display as ipd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6MI6Xb5-9nu"},"source":["We can listen to this artificial mixture."]},{"cell_type":"code","metadata":{"id":"FzJXPO9LXIKy"},"source":["ipd.Audio(mix[0], rate=fs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTuaNzcG_DGX"},"source":["Now, we can instantiate the pretrained SepFormer from <img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/>[HuggingFace](https://huggingface.co/models)<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"20\"/>."]},{"cell_type":"code","metadata":{"id":"8Fmt65bJS1X4"},"source":["from speechbrain.inference.separation import SepformerSeparation\n","separator = SepformerSeparation.from_hparams(source=\"speechbrain/sepformer-wsj02mix\", savedir=\"./pretrained_sepformer\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UfLzNQuhAh9A"},"source":["and we use it to separate the mixture"]},{"cell_type":"code","metadata":{"id":"4CHfWcBTTJhZ"},"source":["est_sources = separator.separate_batch(mix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-IfDAtiWUcp"},"source":["est_sources = est_sources[0] # strip batch dimension"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rvI-xbleAvSj"},"source":["You can listen at the results here:"]},{"cell_type":"code","metadata":{"id":"H6W0qE5OX_v4"},"source":["ipd.Audio(est_sources[:, 0], rate=fs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-GMA0Q7YA62"},"source":["ipd.Audio(est_sources[:, 1], rate=fs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_0v7Ougbgcm"},"source":["## Fine-tuning or using pretrained models as components of a new pipeline"]},{"cell_type":"markdown","metadata":{"id":"RnrPT0GuA03w"},"source":["Here we will show how to fine-tune the CRDNN Encoder Decoder Seq2Seq model used to transcribe the audio in the previous example and downloaded from [here](https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/blob/main/hyperparams.yaml).\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ARD9PaEpE4oD"},"source":["from speechbrain.inference.ASR import EncoderDecoderASR\n","asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"./pretrained_ASR\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuuMQdb4ExyP"},"source":["First we can see that the pretrained `asr_model` allows to access easily all its components:\n"]},{"cell_type":"code","metadata":{"id":"q7c1qywYHkOo"},"source":["asr_model.mods.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5d9JG94JBLs"},"source":["These keys corresponds to the modules entry specified in the [hyperparameter file](https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/blob/main/hyperparams.yaml):\n","\n","\n","```yaml\n","modules:\n","    encoder: !ref <encoder>\n","    decoder: !ref <decoder>\n","    lm_model: !ref <lm_model>\n","```"]},{"cell_type":"markdown","metadata":{"id":"KNbCyQHIJYKx"},"source":["We can also see that the encoder is actually composed of several sub-modules:\n","\n","```yaml\n","encoder: !new:speechbrain.nnet.containers.LengthsCapableSequential\n","    input_shape: [null, null, !ref <n_mels>]\n","    compute_features: !ref <compute_features>\n","    normalize: !ref <normalize>\n","    model: !ref <enc>\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bR6c1k20J3SN"},"source":["These are simply accessible as members of the encoder:"]},{"cell_type":"code","metadata":{"id":"S6Vwi7cNI7eq"},"source":["asr_model.mods.encoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgv5daThU1UC"},"source":["asr_model.mods.encoder.compute_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hoYbLtZmJ8V6"},"source":["The training hyperparameters also can be easily accessed:"]},{"cell_type":"code","metadata":{"id":"fBy1ryJ6H2LJ"},"source":["dir(asr_model.hparams)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUsVgcjgKJKz"},"source":["Such information are pretty useful as we can directly use some of these hyperparameters in our fine-tuning pipeline to ensure compability with the pretrained model (e.g. use the same BOS or EOS indexes)!"]},{"cell_type":"markdown","metadata":{"id":"BDYhruKqLv0G"},"source":["### Setting up the data pipeline"]},{"cell_type":"markdown","metadata":{"id":"Uz8KzuQPEdW-"},"source":["First we must set up the data pipeline for downloaded MiniLibriSpeech data.\n","\n","If you are not familiar with **SpeechBrain dataIO** you may want to take a look at the [tutorial](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH)."]},{"cell_type":"code","metadata":{"id":"MB9zoQGFcjsd"},"source":["import speechbrain as sb\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"axAQJISREGM7"},"source":["We parse MiniLibriSpeech to a suitable JSON annotation"]},{"cell_type":"code","metadata":{"id":"DwcQjRS2cnL5"},"source":["from parse_data import parse_to_json # parse_data is a local library downloaded before (see Installing Dependencies step)\n","parse_to_json(\"./LibriSpeech/dev-clean-2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VIR_XyydEOTS"},"source":["We instantiate a **DynamicItemDataset** from the JSON annotation"]},{"cell_type":"code","metadata":{"id":"kdBfsbdUb3kn"},"source":["from speechbrain.dataio.dataset import DynamicItemDataset\n","dataset = DynamicItemDataset.from_json(\"data.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NEB015VjMDLR"},"source":["We sort the dataset based on length to speed-up training"]},{"cell_type":"code","metadata":{"id":"YSHVrP3CgLq2"},"source":["dataset = dataset.filtered_sorted(sort_key=\"length\", select_n=100)\n","# we limit the dataset to 100 utterances to keep the trainin short in this Colab example"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"npAUJCOEMhXE"},"source":["and add a pipeline for reading audio"]},{"cell_type":"code","metadata":{"id":"Kauu_ke5eglF"},"source":["dataset.add_dynamic_item(sb.dataio.dataio.read_audio, takes=\"file_path\", provides=\"signal\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M1K1xvEHMnmP"},"source":["and another one to encode the words from annotation.\n","\n","It is worth noting that we use the Tokenizer object obtained from the pretrained `asr_model` and  that we encode the words with `asr_model.tokenizer.encode_as_ids(words)`. We also reuse `asr_model` `eos_index` and `bos_index` accessed via `asr_model.hparams` to ensure that all these parameters correspond to the ones used at pretraining time!"]},{"cell_type":"code","metadata":{"id":"bf1rDbYBeh-7"},"source":["# 3. Define text pipeline:\n","@sb.utils.data_pipeline.takes(\"words\")\n","@sb.utils.data_pipeline.provides(\n","        \"words\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\")\n","def text_pipeline(words):\n","      yield words\n","      tokens_list = asr_model.tokenizer.encode_as_ids(words)\n","      yield tokens_list\n","      tokens_bos = torch.LongTensor([asr_model.hparams.bos_index] + (tokens_list))\n","      yield tokens_bos\n","      tokens_eos = torch.LongTensor(tokens_list + [asr_model.hparams.eos_index]) # we use same eos and bos indexes as in pretrained model\n","      yield tokens_eos\n","      tokens = torch.LongTensor(tokens_list)\n","      yield tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZaW1-WEwhDf"},"source":["dataset.add_dynamic_item(text_pipeline)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ijrx8r8OhuH"},"source":["We set the dataset object to return the signal tensor as well as the encoded tokens and words."]},{"cell_type":"code","metadata":{"id":"cAulkwZRfzDo"},"source":["dataset.set_output_keys([\"id\", \"signal\", \"words\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"])\n","dataset[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9-LZusHdOr61"},"source":["### Fine-Tuning the ASR model\n","\n","First, We define our Brain class that will perform the fine-tuning. Here, we just take an example similar to the Brain class of the original [Seq2Seq LibriSpeech recipe](https://github.com/speechbrain/speechbrain/blob/develop/recipes/LibriSpeech/ASR/seq2seq/train.py).\n"]},{"cell_type":"code","metadata":{"id":"EfpGJ3FIgFRu"},"source":["from speechbrain.lobes.features import Fbank\n","import torch\n","\n","# Define fine-tuning procedure\n","class EncDecFineTune(sb.Brain):\n","\n","    def on_stage_start(self, stage, epoch):\n","        # enable grad for all modules we want to fine-tune\n","        if stage == sb.Stage.TRAIN:\n","            for module in [self.modules.enc, self.modules.emb, self.modules.dec, self.modules.seq_lin]:\n","                for p in module.parameters():\n","                    p.requires_grad = True\n","\n","    def compute_forward(self, batch, stage):\n","        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n","        batch = batch.to(self.device)\n","        wavs, wav_lens = batch.signal\n","        tokens_bos, _ = batch.tokens_bos\n","        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n","\n","        # Forward pass\n","        feats = self.modules.compute_features(wavs)\n","        feats = self.modules.normalize(feats, wav_lens)\n","        #feats.requires_grad = True\n","        x = self.modules.enc(feats)\n","\n","        e_in = self.modules.emb(tokens_bos)  # y_in bos + tokens\n","        h, _ = self.modules.dec(e_in, x, wav_lens)\n","\n","        # Output layer for seq2seq log-probabilities\n","        logits = self.modules.seq_lin(h)\n","        p_seq = self.hparams.log_softmax(logits)\n","\n","        return p_seq, wav_lens\n","\n","    def compute_objectives(self, predictions, batch, stage):\n","        \"\"\"Computes the loss (CTC+NLL) given predictions and targets.\"\"\"\n","\n","\n","        p_seq, wav_lens = predictions\n","\n","        ids = batch.id\n","        tokens_eos, tokens_eos_lens = batch.tokens_eos\n","        tokens, tokens_lens = batch.tokens\n","\n","        loss = self.hparams.seq_cost(\n","            p_seq, tokens_eos, tokens_eos_lens)\n","\n","\n","        return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJrCTNIBPVS7"},"source":["Here we define the modules and hyperparameters needed for the Brain class defined before.\n","\n","We fetch them directly from the pretrained model by accessing its `modules` and `hparams`. These can be found in the `hyperparams.yaml` file in the model [HuggingFace repo](https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/blob/main/hyperparams.yaml)."]},{"cell_type":"code","metadata":{"id":"R3B4Dv1Wjfv6"},"source":["modules = {\"enc\": asr_model.mods.encoder.model,\n","           \"emb\": asr_model.hparams.emb,\n","           \"dec\": asr_model.hparams.dec,\n","           \"compute_features\": asr_model.mods.encoder.compute_features, # we use the same features\n","           \"normalize\": asr_model.mods.encoder.normalize,\n","           \"seq_lin\": asr_model.hparams.seq_lin,\n","\n","          }\n","\n","hparams = {\"seq_cost\": lambda x, y, z: speechbrain.nnet.losses.nll_loss(x, y, z, label_smoothing = 0.1),\n","            \"log_softmax\": speechbrain.nnet.activations.Softmax(apply_log=True)}\n","\n","brain = EncDecFineTune(modules, hparams=hparams, opt_class=lambda x: torch.optim.SGD(x, 1e-5))\n","brain.tokenizer = asr_model.tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kqfkX393Rkbb"},"source":["The pre-trained model can be finally fine-tuned:"]},{"cell_type":"code","metadata":{"id":"aHWteDB0jXFp"},"source":["brain.fit(range(2), train_set=dataset,\n","          train_loader_kwargs={\"batch_size\": 8, \"drop_last\":True, \"shuffle\": False})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7qb4lrtG8G-"},"source":["## Pretrainer Class\n","In speechbrain, another way to perform pre-training is to use the PreTrainer Class (`speechbrain.utils.parameter_transfer.Pretrainer`). It orchestrates parameter transfer in a more structured way, which can aid in writing easy-to-share recipes (and it is also central in the implementation `speechbrain.pretrained` models). To use it, let's first initialize a model:"]},{"cell_type":"code","metadata":{"id":"DJ9Wvc41K3p8"},"source":["from speechbrain.lobes.models.ECAPA_TDNN import ECAPA_TDNN\n","\n","model = ECAPA_TDNN(input_size= 80,\n","                   channels= [1024, 1024, 1024, 1024, 3072],\n","                   kernel_sizes= [5, 3, 3, 3, 1],\n","                   dilations= [1, 2, 3, 4, 1],\n","                   attention_channels= 128,\n","                   lin_neurons = 192)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFPMuK0eLRWK"},"source":["At this level, the model is initialized with random parameters. However, we can use our pretrainer to replace random parameters with the ones stored in the saved checkpoint:"]},{"cell_type":"code","metadata":{"id":"PsBfJrc5MhHV"},"source":["from speechbrain.utils.parameter_transfer import Pretrainer\n","\n","# Initialization of the pre-trainer\n","pretrain = Pretrainer(loadables={'model': model}, paths={'model': 'speechbrain/spkrec-ecapa-voxceleb/embedding_model.ckpt'})\n","\n","# We download the pretrained model from HuggingFace in this case\n","pretrain.collect_files()\n","pretrain.load_collected()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBxnWXe4OYSJ"},"source":["Now, the model is not anymore randomly initialized, but it contains the pre-trained parameters of `embedding_model.ckpt`.  The path of the pre-trained model can be a **local path**, a **web url**, or a **huggingface repository**:"]},{"cell_type":"code","metadata":{"id":"4AQW7yD3PRLD"},"source":["# Local Path\n","pretrain = Pretrainer(collect_in='model_local', loadables={'model': model}, paths={'model': 'model_checkpoints/model.ckpt'})\n","pretrain.collect_files()\n","pretrain.load_collected()\n","\n","# Or web\n","pretrain = Pretrainer(collect_in='model_web', loadables={'model': model}, paths={'model': 'https://www.dropbox.com/s/2mdnl784ram5w8o/embedding_model.ckpt?dl=1'})\n","pretrain.collect_files()\n","pretrain.load_collected()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BBlIHzVjRHHA"},"source":["As you can see, you can use the variable `collect_in` to set where the pre-trained model is stored."]},{"cell_type":"markdown","metadata":{"id":"OyjEzG-6zIFT"},"source":["# Acknowledgements\n","\n","\n","*   Many thanks to ([ziz19](https://github.com/ziz19)) who helped improving this Tutorial.\n"]},{"cell_type":"markdown","metadata":{"id":"rbe9wxuMEOBz"},"source":["# **About SpeechBrain**\n","- Website: https://speechbrain.github.io/\n","- Code: https://github.com/speechbrain/speechbrain/\n","- HuggingFace: https://huggingface.co/speechbrain/\n","\n","\n","# **Citing SpeechBrain**\n","Please, cite SpeechBrain if you use it for your research or business.\n","\n","```bibtex\n","@misc{speechbrain,\n","  title={SpeechBrain: A General-Purpose Speech Toolkit},\n","  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n","  year={2021},\n","  eprint={2106.04624},\n","  archivePrefix={arXiv},\n","  primaryClass={eess.AS}\n","}\n","```"]}]}